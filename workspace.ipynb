{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer_Attention(nn.Module):\n",
    "    \"\"\"One head of self/cross-attention\"\"\"\n",
    "    def __init__(self, embed_dim:int, masked:bool, dropout:float, kdim:int, vdim:int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.WQ = nn.Linear(embed_dim, kdim, bias=False)\n",
    "        self.WK = nn.Linear(embed_dim, kdim, bias=False)\n",
    "        self.WV = nn.Linear(embed_dim, vdim, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.masked = masked\n",
    "\n",
    "    def forward(self, *x):\n",
    "        # x[0] for WQ\n",
    "        # x[1] or x[-1] for WK and WV\n",
    "        assert x[0].size(-1) == x[-1].size(-1), \"mismatched shapes\"\n",
    "        q = self.WQ(x[0]) # (B,T,kdim)\n",
    "        k = self.WK(x[-1]) # (B,T,kdim)\n",
    "\n",
    "        attention_act = q @ k.transpose(1, 2) * k.size(-1) ** -0.5\n",
    "        attention_weights = self.dropout(F.softmax(attention_act, dim=-1)) # (B,T,T)\n",
    "\n",
    "        if self.masked:\n",
    "            tril = torch.tril(torch.ones_like(attention_weights, requires_grad=False)).to(x[-1].device)\n",
    "            attention_weights.masked_fill(tril==0, float(\"-inf\"))\n",
    "\n",
    "        v = self.WV(x[-1]) # (B,T,vdim)\n",
    "        ## Weighted sum\n",
    "        out = attention_weights @ v\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHead_TransformerAttention(nn.Module):\n",
    "    \"\"\"Multiple heads of self/cross-attention in parallel\"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, masked:bool, dropout=0.0, kdim=None, vdim=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        if not kdim:\n",
    "            kdim = embed_dim\n",
    "\n",
    "        if not vdim:\n",
    "            vdim = embed_dim\n",
    "        self.head_list = nn.ModuleList([Transformer_Attention(embed_dim, masked, dropout, kdim, vdim) for _ in range(num_heads)])\n",
    "        self.WO = nn.Linear(num_heads*vdim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, *x):\n",
    "        y = torch.cat([head(*x) for head in self.head_list], dim=-1)\n",
    "        \n",
    "        y = self.dropout(self.WO(y))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward_Network(nn.Module):\n",
    "    def __init__(self, d_model, dim_feedforward, dropout):\n",
    "        super().__init__()\n",
    "        self.ff = nn.Sequential(nn.Linear(d_model, dim_feedforward),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(dim_feedforward, d_model),\n",
    "                                nn.Dropout(dropout))\n",
    "    def forward(self, x):\n",
    "        return self.ff(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EncoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model:int, nhead:int, dim_feedforward:int, dropout=0.0):\n",
    "        super().__init__()\n",
    "        head_features = d_model // nhead\n",
    "        self.mha = MultiHead_TransformerAttention(d_model, nhead, False, dropout, head_features, head_features)\n",
    "        self.ln_mha = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.ffn = FeedForward_Network(d_model, dim_feedforward, dropout)\n",
    "        self.ln_ffn = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ## (x +) is for residual connections\n",
    "        ## It observed that doing layernorm before computation layers is better, but for now we only do re-implementing for the paper which do layernorm after computation layer.\n",
    "        x = self.ln_mha(x + self.mha(x))\n",
    "        x = self.ln_ffn(x + self.ffn(x))\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_layers: int, d_model:int, nhead:int, dim_feedforward:int, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.Sequential(*[EncoderLayer(d_model, nhead, dim_feedforward, dropout) for _ in range(num_layers)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.blocks(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DecoderLayer\n",
    "- There are two types of decoders:\n",
    "    1. Decoder that works as auto-regressive, and this decoder will have only self-attention layers.\n",
    "    2. Decoder that works with the Encoder, and this decoder will have self-attention layers followed my cross-attention layers (like in the paper).\n",
    "- For this repo we are going to implement cross-attention decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model:int, nhead:int, dim_feedforward:int, dropout=0.0):\n",
    "        super().__init__()\n",
    "        head_features = d_model // nhead\n",
    "\n",
    "        self.self_mha = MultiHead_TransformerAttention(d_model, nhead, True, dropout, kdim=head_features, vdim=head_features)\n",
    "        self.ln_self_mha = nn.LayerNorm(d_model)\n",
    "\n",
    "        ## Cross attention\n",
    "        self.cross_mha = MultiHead_TransformerAttention(d_model, nhead, False, dropout, kdim=head_features, vdim=head_features)\n",
    "        self.ln_cross_mha = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.ffn = FeedForward_Network(d_model, dim_feedforward, dropout)\n",
    "        self.ln_ffn = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, *x):\n",
    "        # x[0] should be decoder input embedings\n",
    "        # x[1] should be encoder output embedings\n",
    "        x_dec = x[0]\n",
    "        x_enc = x[1] ## x[1] instead of x[-1] to raise an error incase user forgot to pass encoder rich-tokens\n",
    "\n",
    "        x_dec = self.ln_self_mha(x_dec + self.self_mha(x_dec))\n",
    "        oo = self.ln_cross_mha(x_dec + self.cross_mha(x_dec, x_enc))\n",
    "        oo = self.ln_ffn(oo + self.ffn(oo))\n",
    "        return oo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_layers: int, d_model:int, nhead:int, dim_feedforward:int, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.Sequential(*[DecoderLayer(d_model, nhead, dim_feedforward, dropout) for _ in range(num_layers)])\n",
    "        \n",
    "    def forward(self, *x):\n",
    "        x_dec = x[0]\n",
    "        x_enc = x[1]\n",
    "        for block in self.blocks:\n",
    "            x_dec = block(x_dec, x_enc)\n",
    "        return x_dec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder-Decoder Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparameters (for test)\n",
    "layers_repeat = 6\n",
    "heads = 8 ## number of heads for each multi-head Attention\n",
    "d_model = 512\n",
    "dropout = 0.1\n",
    "dim_feedforward = d_model*4\n",
    "enc_vocab_size = 30000\n",
    "dec_vocab_size = 40000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.zeros_like(x)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encEmbed = nn.Embedding(enc_vocab_size, d_model)\n",
    "        self.decEmbed = nn.Embedding(dec_vocab_size, d_model)\n",
    "        self.pos = Identity() # Pass for now\n",
    "\n",
    "        self.encoder = Encoder(num_layers=layers_repeat, d_model=d_model,\n",
    "                                nhead=heads, dim_feedforward=dim_feedforward, dropout=dropout)\n",
    "        \n",
    "        self.decoder = Decoder(num_layers=layers_repeat, d_model=d_model,\n",
    "                                nhead=heads, dim_feedforward=dim_feedforward, dropout=dropout)\n",
    "\n",
    "        self.classifier = nn.Linear(d_model, dec_vocab_size)\n",
    "    \n",
    "    def forward(self, src_tokens, trgt_tokens):\n",
    "\n",
    "        src_embed = self.encEmbed(src_tokens)\n",
    "        src_pos = self.pos(src_embed)\n",
    "        encoder_input = src_embed + src_pos\n",
    "        encoder_context = self.encoder(encoder_input)\n",
    "        \n",
    "        trgt_embed = self.decEmbed(trgt_tokens)\n",
    "        trgt_pos = self.pos(trgt_embed)\n",
    "        decoder_input = trgt_embed + trgt_pos\n",
    "        decoder_context = self.decoder(decoder_input, encoder_context)\n",
    "        \n",
    "        logits = self.classifier(decoder_context)\n",
    "        return logits\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def inference(self, src_tokens):\n",
    "        src_embed = self.encEmbed(src_tokens)\n",
    "        src_pos = self.pos(src_embed)\n",
    "        encoder_input = src_embed + src_pos\n",
    "        encoder_context = self.encoder(encoder_input)\n",
    "\n",
    "        ## Assume <pad>:0, <unk>:1 <s>:2, </s>:3\n",
    "        token_list = [2]\n",
    "        confidences = []\n",
    "        token = token_list[0]\n",
    "        maxtries = 0\n",
    "\n",
    "        while token != 3 and maxtries <= src_tokens.size(-1) + 5:\n",
    "            trgt_embed = self.decEmbed(torch.tensor([token_list]).to(src_tokens.device))\n",
    "            trgt_pos = self.pos(trgt_embed)\n",
    "            decoder_input = trgt_embed + trgt_pos\n",
    "            decoder_context = self.decoder(decoder_input, encoder_context)\n",
    "            logits = self.classifier(decoder_context) # (B,T,vocab_size) often B=1\n",
    "            \n",
    "            softmax = F.softmax(logits[:,-1,:], dim=-1) # (B,vocab_size)\n",
    "            confidence, token = torch.max(softmax, dim=-1)\n",
    "            \n",
    "            token = token.item()\n",
    "            confidence = confidence.item()\n",
    "            token_list.append(token)\n",
    "            confidences.append(confidence)\n",
    "            maxtries += 1\n",
    "        \n",
    "        return token_list, confidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 15, 40000])\n"
     ]
    }
   ],
   "source": [
    "transformer = Transformer()\n",
    "src_tokens = torch.randint(low=0, high=enc_vocab_size, size=(32, 10))\n",
    "trgt_tokens = torch.randint(low=0, high=enc_vocab_size, size=(32, 15))\n",
    "\n",
    "logits = transformer(src_tokens, trgt_tokens)\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 24271, 37332, 18889, 26387, 19201, 34973, 17613, 26224, 18935, 20685, 20685, 20685, 20685, 20685]\n",
      "[0.00022207263100426644, 0.0003712518373504281, 0.0003272329340688884, 0.00031365477479994297, 0.0002349430724279955, 0.00019178724323865026, 0.00018867503968067467, 0.00022262873244471848, 0.00023418100317940116, 0.00020738778403028846, 0.00019847130170091987, 0.0002041447296505794, 0.00020849690190516412, 0.0002112431247951463]\n",
      "Not Trained yet.\n"
     ]
    }
   ],
   "source": [
    "x_text = torch.tensor([[2,500,100,8564, 21, 1, 754, 3]]) ## dumb\n",
    "transformer.eval()\n",
    "tokens, confidences = transformer.inference(x_text)\n",
    "transformer.train()\n",
    "print(tokens, confidences, sep='\\n')\n",
    "print(\"Not Trained yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters= 100,470,848\n",
      "Total non-trainable parameters= 0\n"
     ]
    }
   ],
   "source": [
    "def get_parameters_info(model):\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad==True)\n",
    "    nontrainable = sum(p.numel() for p in model.parameters() if p.requires_grad==False)\n",
    "\n",
    "    return trainable, nontrainable\n",
    "\n",
    "tr, nontr = get_parameters_info(transformer)\n",
    "print(f\"Total trainable parameters= {tr:,}\\nTotal non-trainable parameters= {nontr:,}\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
